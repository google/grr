#!/usr/bin/env python
"""Classes for exporting data from AFF4 to the rest of the world.

Exporters defined here convert various complex RDFValues to simple RDFValues
(without repeated fields, without recursive field definitions) that can
easily be written to a relational database or just to a set of files.
"""

import hashlib
import stat
import time

from google.protobuf import descriptor_pb2
from google.protobuf import descriptor
from google.protobuf import message_factory

import logging


from grr.lib import aff4
from grr.lib import rdfvalue
from grr.lib import registry
from grr.lib import type_info
from grr.lib import utils
from grr.lib.aff4_objects import filestore
from grr.proto import export_pb2


class Error(Exception):
  """Errors generated by export converters."""


class NoConverterFound(Error):
  """Raised when no converter is found for particular value."""


class ExportOptions(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportOptions


class ExportedMetadata(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedMetadata

  def __init__(self, initializer=None, age=None, payload=None, **kwarg):
    super(ExportedMetadata, self).__init__(initializer=initializer,
                                           age=age, **kwarg)

    if not self.timestamp:
      self.timestamp = rdfvalue.RDFDatetime().Now()


class ExportedClient(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedClient


class ExportedFile(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedFile


class ExportedRegistryKey(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedRegistryKey


class ExportedProcess(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedProcess


class ExportedNetworkConnection(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedNetworkConnection


class ExportedDNSClientConfiguration(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedDNSClientConfiguration


class ExportedOpenFile(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedOpenFile


class ExportedNetworkInterface(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedNetworkInterface


class ExportedFileStoreHash(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedFileStoreHash


class ExportedSoftware(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedSoftware


class ExportedMatch(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedMatch


class ExportConverter(object):
  """Base ExportConverter class.

  ExportConverters are used to convert RDFValues to export-friendly RDFValues.
  "Export-friendly" means 2 things:
    * Flat structure
    * No repeated fields (i.e. lists)

  In order to use ExportConverters, users have to use ConvertValues.
  These methods will look up all the available ExportConverters descendants
  and will choose the ones that have input_rdf_type attribute equal to the
  type of the values being converted. It's ok to have multiple converters with
  the same input_rdf_type value. They will be applied sequentially and their
  cumulative results will be returned.
  """

  __metaclass__ = registry.MetaclassRegistry

  # Type of values that this converter accepts.
  input_rdf_type = None

  # Cache used for GetConvertersByValue() lookups.
  converters_cache = {}

  def __init__(self, options=None):
    """Constructor.

    Args:
      options: ExportOptions value, which contains settings that may or
               or may not affect this converter's behavior.
    """
    super(ExportConverter, self).__init__()
    self.options = options or rdfvalue.ExportOptions()

  def Convert(self, metadata, value, token=None):
    """Converts given RDFValue to other RDFValues.

    Metadata object is provided by the caller. It contains basic information
    about where the value is coming from (i.e. client_urn, session_id, etc)
    as well as timestamps corresponding to when data was generated and
    exported.

    ExportConverter should use the metadata when constructing export-friendly
    RDFValues.

    Args:
      metadata: ExportedMetadata to be used for conversion.
      value: RDFValue to be converted.
      token: Security token.

    Yields:
      Resulting RDFValues. Empty list is a valid result and means that
      conversion wasn't possible. Resulting RDFValues may be of different
      types.
    """
    raise NotImplementedError()

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Converts a batch of RDFValues at once.

    This is a default non-optimized dumb implementation. Subclasses are
    supposed to have their own optimized implementations.

    Metadata object is provided by the caller. It contains basic information
    about where the value is coming from (i.e. client_urn, session_id, etc)
    as well as timestamps corresponding to when data was generated and
    exported.

    ExportConverter should use the metadata when constructing export-friendly
    RDFValues.

    Args:
      metadata_value_pairs: a list or a generator of tuples (metadata, value),
                            where metadata is ExportedMetadata to be used for
                            conversion and value is an RDFValue to be converted.
      token: Security token.

    Yields:
      Resulting RDFValues. Empty list is a valid result and means that
      conversion wasn't possible. Resulting RDFValues may be of different
      types.
    """
    for metadata, value in metadata_value_pairs:
      for result in self.Convert(metadata, value, token):
        yield result

  @staticmethod
  def GetConvertersByValue(value):
    """Returns all converters that take given value as an input value."""
    try:
      return ExportConverter.converters_cache[value.__class__.__name__]
    except KeyError:
      results = [cls for cls in ExportConverter.classes.itervalues()
                 if cls.input_rdf_type == value.__class__.__name__]
      if not results:
        results = [DataAgnosticExportConverter]

      ExportConverter.converters_cache[value.__class__.__name__] = results
      return results


class DataAgnosticExportConverter(ExportConverter):
  """Export converter that yields flattened versions of passed values.

  NOTE: DataAgnosticExportConverter discards complex types: repeated
  fields and nested messages. Only the primitive types (including enums)
  are preserved.
  """

  PRIMITIVE_TYPE_MAPPING = {
      "string": descriptor_pb2.FieldDescriptorProto.TYPE_STRING,
      "bytes": descriptor_pb2.FieldDescriptorProto.TYPE_BYTES,
      "uint64": descriptor_pb2.FieldDescriptorProto.TYPE_UINT64,
      "int64": descriptor_pb2.FieldDescriptorProto.TYPE_INT32,
      "float": descriptor_pb2.FieldDescriptorProto.TYPE_FLOAT,
      "double": descriptor_pb2.FieldDescriptorProto.TYPE_DOUBLE,
      "bool": descriptor_pb2.FieldDescriptorProto.TYPE_BOOL
      }

  # Cache used for generated classes.
  classes_cache = {}

  def ExportedClassNameForValue(self, value):
    return "Exported" + value.__class__.__name__

  def MakeDescriptor(self, desc_proto, file_desc_proto, descriptors=None):
    """Creates a protobuf descriptor out of DescriptorProto."""
    descriptors = descriptors or dict()
    full_message_name = [desc_proto.name]

    file_descriptor = descriptor.FileDescriptor(
        file_desc_proto.name, file_desc_proto.package,
        serialized_pb=file_desc_proto.SerializeToString())

    # Create Descriptors for enum types
    enum_types = {}
    for enum_proto in desc_proto.enum_type:
      full_name = ".".join(full_message_name + [enum_proto.name])

      values = []
      for index, enum_val in enumerate(enum_proto.value):
        values.append(descriptor.EnumValueDescriptor(
            enum_val.name, index, enum_val.number))

      enum_desc = descriptor.EnumDescriptor(enum_proto.name, full_name,
                                            None, values)
      enum_types[full_name] = enum_desc

    fields = []
    for field_proto in desc_proto.field:
      full_name = ".".join(full_message_name + [field_proto.name])
      enum_desc = None
      message_desc = None
      if field_proto.HasField("type_name"):
        type_name = field_proto.type_name
        full_type_name = ".".join(full_message_name +
                                  [type_name[type_name.rfind(".") + 1:]])

        if full_type_name in enum_types:
          enum_desc = enum_types[full_type_name]
        elif type_name in descriptors:
          message_desc = descriptors[type_name]

      # Else type_name references a non-local type, which isn't implemented
      field = descriptor.FieldDescriptor(
          field_proto.name, full_name, field_proto.number - 1,
          field_proto.number, field_proto.type,
          descriptor.FieldDescriptor.ProtoTypeToCppProtoType(field_proto.type),
          field_proto.label, None, message_desc, enum_desc, None, False, None,
          options=field_proto.options, has_default_value=False)
      fields.append(field)

    desc_name = ".".join(full_message_name)
    return descriptor.Descriptor(desc_proto.name, desc_name, None, None, fields,
                                 [], enum_types.values(), [],
                                 file=file_descriptor)

  def MakeFlatRDFClass(self, value):
    """Generates flattened RDFValue class definition for the given value."""
    file_descriptor = descriptor_pb2.FileDescriptorProto()
    file_descriptor.name = (self.ExportedClassNameForValue(value).lower() +
                            ".proto")

    descriptors = dict()
    descriptors[
        "." + rdfvalue.ExportedMetadata.protobuf.DESCRIPTOR.full_name] = (
            rdfvalue.ExportedMetadata.protobuf.DESCRIPTOR)
    # Register import of a proto file containing ExportedMetadata definiition.
    file_descriptor.dependency.append(
        rdfvalue.ExportedMetadata.protobuf.DESCRIPTOR.file.name)

    message_type = file_descriptor.message_type.add()
    message_type.name = self.ExportedClassNameForValue(value)

    metadata_field = message_type.field.add()
    metadata_field.name = "metadata"
    metadata_field.number = 1
    metadata_field.label = descriptor_pb2.FieldDescriptorProto.LABEL_OPTIONAL
    metadata_field.type = descriptor_pb2.FieldDescriptorProto.TYPE_MESSAGE
    metadata_field.type_name = (
        "." + rdfvalue.ExportedMetadata.protobuf.DESCRIPTOR.full_name)

    for number, desc in sorted(value.type_infos_by_field_number.items()):
      # Name 'metadata' is reserved to store ExportedMetadata value.
      if desc.name == "metadata":
        continue

      field = None
      if isinstance(desc, type_info.ProtoEnum):
        field = message_type.field.add()
        field.type = descriptor_pb2.FieldDescriptorProto.TYPE_ENUM
        field.type_name = desc.enum_name

        enum_type = message_type.enum_type.add()
        value.protobuf.DESCRIPTOR.enum_types_by_name[
            desc.enum_name].CopyToProto(enum_type)
      elif isinstance(desc, type_info.ProtoEmbedded):
        # We don't support nested protobufs in data agnostic export yet.
        pass
      elif isinstance(desc, type_info.ProtoList):
        # We don't support repeated fields in data agnostic export yet.
        pass
      else:
        field = message_type.field.add()
        field.type = self.PRIMITIVE_TYPE_MAPPING[desc.proto_type_name]

      if field:
        field.name = desc.name
        field.number = number + 1
        field.label = descriptor_pb2.FieldDescriptorProto.LABEL_OPTIONAL

        field_options = value.protobuf.DESCRIPTOR.fields_by_name[
            desc.name].GetOptions()
        if field_options:
          field.options.CopyFrom(field_options)

    result_descriptor = self.MakeDescriptor(message_type, file_descriptor,
                                            descriptors=descriptors)

    factory = message_factory.MessageFactory()
    proto_class = factory.GetPrototype(result_descriptor)

    def Flatten(self, metadata, value):
      self.metadata = metadata
      for desc in value.type_infos:
        if desc.name == "metadata":
          continue
        if hasattr(self, desc.name) and value.HasField(desc.name):
          setattr(self, desc.name, getattr(value, desc.name))

    return type(utils.SmartStr(message_type.name),
                (rdfvalue.RDFProtoStruct,),
                dict(protobuf=proto_class, Flatten=Flatten))

  def Convert(self, metadata, value, token=None):
    class_name = self.ExportedClassNameForValue(value)
    try:
      class_obj = DataAgnosticExportConverter.classes_cache[class_name]
    except KeyError:
      class_obj = self.MakeFlatRDFClass(value)
      DataAgnosticExportConverter.classes_cache[class_name] = class_obj

    result_obj = class_obj()
    result_obj.Flatten(metadata, value)
    yield result_obj

  def BatchConvert(self, metadata_value_pairs, token=None):
    for metadata, value in metadata_value_pairs:
      for result in self.Convert(metadata, value, token=token):
        yield result


class StatEntryToExportedFileConverter(ExportConverter):
  """Converts StatEntry to ExportedFile."""

  input_rdf_type = "StatEntry"

  MAX_CONTENT_SIZE = 1024 * 64

  @staticmethod
  def ParseSignedData(signed_data, result):
    """Parses signed certificate data and updates result rdfvalue."""

  @staticmethod
  def ParseFileHash(hash_obj, result):
    """Parses Hash rdfvalue into ExportedFile's fields."""
    if hash_obj.HasField("md5"):
      result.hash_md5 = str(hash_obj.md5)

    if hash_obj.HasField("sha1"):
      result.hash_sha1 = str(hash_obj.sha1)

    if hash_obj.HasField("sha256"):
      result.hash_sha256 = str(hash_obj.sha256)

    if hash_obj.HasField("pecoff_md5"):
      result.pecoff_hash_md5 = str(hash_obj.pecoff_md5)

    if hash_obj.HasField("pecoff_sha1"):
      result.pecoff_hash_sha1 = str(hash_obj.pecoff_sha1)

    if hash_obj.HasField("signed_data"):
      StatEntryToExportedFileConverter.ParseSignedData(
          hash_obj.signed_data[0], result)

  def Convert(self, metadata, stat_entry, token=None):
    """Converts StatEntry to ExportedFile.

    Does nothing if StatEntry corresponds to a registry entry and not to a file.

    Args:
      metadata: ExportedMetadata to be used for conversion.
      stat_entry: StatEntry to be converted.
      token: Security token.

    Returns:
      List or generator with resulting RDFValues. Empty list if StatEntry
      corresponds to a registry entry and not to a file.
    """
    return self.BatchConvert([(metadata, stat_entry)], token=token)

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Converts a batch of StatEntry value to ExportedFile values at once.

    Args:
      metadata_value_pairs: a list or a generator of tuples (metadata, value),
                            where metadata is ExportedMetadata to be used for
                            conversion and value is a StatEntry to be converted.
      token: Security token:

    Yields:
      Resulting ExportedFile values. Empty list is a valid result and means that
      conversion wasn't possible.
    """
    filtered_pairs = []
    for metadata, stat_entry in metadata_value_pairs:
      # Ignore registry keys.
      if stat_entry.pathspec.pathtype != rdfvalue.PathSpec.PathType.REGISTRY:
        filtered_pairs.append((metadata, stat_entry))

    if self.options.export_files_hashes or self.options.export_files_contents:
      aff4_paths = [stat_entry.aff4path
                    for metadata, stat_entry in metadata_value_pairs]
      fds = aff4.FACTORY.MultiOpen(aff4_paths, mode="r", token=token)
      fds_dict = dict([(fd.urn, fd) for fd in fds])

    for metadata, stat_entry in filtered_pairs:
      result = ExportedFile(metadata=metadata,
                            urn=stat_entry.aff4path,
                            basename=stat_entry.pathspec.Basename(),
                            st_mode=stat_entry.st_mode,
                            st_ino=stat_entry.st_ino,
                            st_dev=stat_entry.st_dev,
                            st_nlink=stat_entry.st_nlink,
                            st_uid=stat_entry.st_uid,
                            st_gid=stat_entry.st_gid,
                            st_size=stat_entry.st_size,
                            st_atime=stat_entry.st_atime,
                            st_mtime=stat_entry.st_mtime,
                            st_ctime=stat_entry.st_ctime,
                            st_blocks=stat_entry.st_blocks,
                            st_blksize=stat_entry.st_blksize,
                            st_rdev=stat_entry.st_rdev,
                            symlink=stat_entry.symlink)

      if self.options.export_files_hashes or self.options.export_files_contents:
        try:
          aff4_object = fds_dict[stat_entry.aff4path]

          if self.options.export_files_hashes:
            hash_obj = aff4_object.Get(aff4_object.Schema.HASH)
            if hash_obj:
              self.ParseFileHash(hash_obj, result)

          if self.options.export_files_contents:
            try:
              result.content = aff4_object.Read(self.MAX_CONTENT_SIZE)
              result.content_sha256 = hashlib.sha256(result.content).hexdigest()
            except (IOError, AttributeError) as e:
              logging.warning("Can't read content of %s: %s",
                              stat_entry.aff4path, e)
        except KeyError:
          pass

      result.metadata.annotations = u",".join(self.options.annotations)

      yield result


class StatEntryToExportedRegistryKeyConverter(ExportConverter):
  """Converts StatEntry to ExportedRegistryKey."""

  input_rdf_type = "StatEntry"

  def Convert(self, metadata, stat_entry, token=None):
    """Converts StatEntry to ExportedRegistryKey.

    Does nothing if StatEntry corresponds to a file and nto a registry entry.

    Args:
      metadata: ExportedMetadata to be used for conversion.
      stat_entry: StatEntry to be converted.
      token: Security token.

    Returns:
      List or generator with resulting RDFValues. Empty list if StatEntry
      corresponds to a file and not to a registry entry.
    """
    if stat_entry.pathspec.pathtype != rdfvalue.PathSpec.PathType.REGISTRY:
      return []

    result = ExportedRegistryKey(metadata=metadata,
                                 urn=stat_entry.aff4path,
                                 last_modified=stat_entry.st_mtime)

    if (stat_entry.HasField("registry_type") and
        stat_entry.HasField("registry_data")):

      result.type = stat_entry.registry_type
      try:
        data = str(stat_entry.registry_data.GetValue())
      except UnicodeEncodeError:
        # If we can't represent this as a string...
        # let's just get the byte representation *shrug*
        data = stat.registry_data.GetValue()
        # Get the byte representation of the string
        data = unicode(data).encode("utf-16be")

      result.data = data

    return [result]


class ProcessToExportedProcessConverter(ExportConverter):
  """Converts Process to ExportedProcess."""

  input_rdf_type = "Process"

  def Convert(self, metadata, process, token=None):
    """Converts Process to ExportedProcess."""

    result = ExportedProcess(metadata=metadata,
                             pid=process.pid,
                             ppid=process.ppid,
                             name=process.name,
                             exe=process.exe,
                             cmdline=" ".join(process.cmdline),
                             ctime=process.ctime,
                             real_uid=process.real_uid,
                             effective_uid=process.effective_uid,
                             saved_uid=process.saved_uid,
                             real_gid=process.real_gid,
                             effective_gid=process.effective_gid,
                             saved_gid=process.saved_gid,
                             username=process.username,
                             terminal=process.terminal,
                             status=process.status,
                             nice=process.nice,
                             cwd=process.cwd,
                             num_threads=process.num_threads,
                             user_cpu_time=process.user_cpu_time,
                             system_cpu_time=process.system_cpu_time,
                             cpu_percent=process.cpu_percent,
                             rss_size=process.RSS_size,
                             vms_size=process.VMS_size,
                             memory_percent=process.memory_percent)
    return [result]


class ProcessToExportedNetworkConnectionConverter(ExportConverter):
  """Converts Process to ExportedNetworkConnection."""

  input_rdf_type = "Process"

  def Convert(self, metadata, process, token=None):
    """Converts Process to ExportedNetworkConnection."""

    for conn in process.connections:
      yield ExportedNetworkConnection(metadata=metadata,
                                      family=conn.family,
                                      type=conn.type,
                                      local_address=conn.local_address,
                                      remote_address=conn.remote_address,
                                      state=conn.state,
                                      pid=conn.pid,
                                      ctime=conn.ctime)


class ProcessToExportedOpenFileConverter(ExportConverter):
  """Converts Process to ExportedOpenFile."""

  input_rdf_type = "Process"

  def Convert(self, metadata, process, token=None):
    """Converts Process to ExportedOpenFile."""

    for f in process.open_files:
      yield ExportedOpenFile(metadata=metadata,
                             pid=process.pid,
                             path=f)


class SoftwareToExportedSoftwareConverter(ExportConverter):
  """Converts Software to ExportedSoftware."""

  input_rdf_type = "Software"

  def Convert(self, metadata, software, token=None):
    yield ExportedSoftware(metadata=metadata,
                           software=software)


class InterfaceToExportedNetworkInterfaceConverter(ExportConverter):
  input_rdf_type = "Interface"

  def Convert(self, metadata, interface, token=None):
    """Converts Interface to ExportedNetworkInterfaces."""
    ip4_addresses = []
    ip6_addresses = []
    for addr in interface.addresses:
      if addr.address_type == addr.Family.INET:
        ip4_addresses.append(addr.human_readable_address)
      elif addr.address_type == addr.Family.INET6:
        ip6_addresses.append(addr.human_readable_address)
      else:
        raise ValueError("Invalid address type: %s", addr.address_type)

    result = ExportedNetworkInterface(
        metadata=metadata,
        ifname=interface.ifname,
        ip4_addresses=" ".join(ip4_addresses),
        ip6_addresses=" ".join(ip6_addresses))

    if interface.mac_address:
      result.mac_address = interface.mac_address.human_readable_address

    yield result


class DNSClientConfigurationToExportedDNSClientConfiguration(ExportConverter):
  input_rdf_type = "DNSClientConfiguration"

  def Convert(self, metadata, config, token=None):
    """Converts DNSClientConfiguration to ExportedDNSClientConfiguration."""
    result = ExportedDNSClientConfiguration(
        metadata=metadata,
        dns_servers=" ".join(config.dns_server),
        dns_suffixes=" ".join(config.dns_suffix))
    yield result


class ClientSummaryToExportedNetworkInterfaceConverter(
    InterfaceToExportedNetworkInterfaceConverter):
  input_rdf_type = "ClientSummary"

  def Convert(self, metadata, client_summary, token=None):
    """Converts ClientSummary to ExportedNetworkInterfaces."""
    for interface in client_summary.interfaces:
      yield super(
          ClientSummaryToExportedNetworkInterfaceConverter, self).Convert(
              metadata, interface, token=token).next()


class ClientSummaryToExportedClientConverter(ExportConverter):
  input_rdf_type = "ClientSummary"

  def Convert(self, metadata, unused_client_summary, token=None):
    return [ExportedClient(metadata=metadata)]


class BufferReferenceToExportedMatchConverter(ExportConverter):
  """Export converter for BufferReference instances."""

  input_rdf_type = "BufferReference"

  def Convert(self, metadata, buffer_reference, token=None):
    yield ExportedMatch(metadata=metadata,
                        offset=buffer_reference.offset,
                        length=buffer_reference.length,
                        data=buffer_reference.data,
                        urn=aff4.AFF4Object.VFSGRRClient.PathspecToURN(
                            buffer_reference.pathspec,
                            metadata.client_urn))


class FileFinderResultConverter(ExportConverter):
  """Export converter for FileFinderResult instances."""

  input_rdf_type = "FileFinderResult"

  def BatchConvert(self, metadata_value_pairs, token=None):
    for result in ConvertValuesWithMetadata(
        [(metadata, val.stat_entry) for metadata, val in metadata_value_pairs],
        token=token, options=self.options):

      # FileFinderResult has hashes in "hash_entry" attribute which is not
      # passed to ConvertValuesWithMetadata call. We have to process these
      # data explicitly here. Note also that we only do this when
      # ConvertValuesWithMetadata produces ExportedFile values. We have to
      # check for the value type explicitly as ConvertValuesWithMetadata
      # may produce values of different types.
      if val.HasField("hash_entry") and isinstance(result, ExportedFile):
        StatEntryToExportedFileConverter.ParseFileHash(val.hash_entry, result)

      yield result

    matches = []
    for metadata, val in metadata_value_pairs:
      matches.extend([(metadata, match) for match in val.matches])

    for result in ConvertValuesWithMetadata(matches, token=token,
                                            options=self.options):
      yield result

  def Convert(self, metadata, result, token=None):
    return self.BatchConvert([(metadata, result)], token=token)


class RDFURNConverter(ExportConverter):
  """Follows RDFURN and converts its target object into a set of RDFValues.

  If urn points to a RDFValueCollection, RDFURNConverter goes through the
  collection and converts every value there. If urn points to an object
  with "STAT" attribute, it converts just that attribute.
  """

  input_rdf_type = "RDFURN"

  def Convert(self, metadata, stat_entry, token=None):
    return self.BatchConvert([(metadata, stat_entry)], token=token)

  def BatchConvert(self, metadata_value_pairs, token=None):
    urn_metadata_pairs = []
    for metadata, value in metadata_value_pairs:
      if isinstance(value, rdfvalue.RDFURN):
        urn_metadata_pairs.append((value, metadata))

    urns_dict = dict(urn_metadata_pairs)
    fds = aff4.FACTORY.MultiOpen(urns_dict.iterkeys(), mode="r", token=token)

    batch = []
    for fd in fds:
      batch.append((urns_dict[fd.urn], fd))

    try:
      return ConvertValuesWithMetadata(batch)
    except NoConverterFound as e:
      logging.debug(e)

    return []


class RDFValueCollectionConverter(ExportConverter):

  input_rdf_type = "RDFValueCollection"

  BATCH_SIZE = 1000

  def Convert(self, metadata, collection, token=None):
    if not collection:
      return

    for batch in utils.Grouper(collection, self.BATCH_SIZE):
      converted_batch = ConvertValues(metadata, batch, token=token,
                                      options=self.options)
      for v in converted_batch:
        yield v


class VFSFileToExportedFileConverter(ExportConverter):

  input_rdf_type = "VFSFile"

  def Convert(self, metadata, vfs_file, token=None):
    stat_entry = vfs_file.Get(vfs_file.Schema.STAT)
    if not stat_entry:
      return []

    result = ExportedFile(metadata=metadata,
                          urn=stat_entry.aff4path,
                          basename=stat_entry.pathspec.Basename(),
                          st_mode=stat_entry.st_mode,
                          st_ino=stat_entry.st_ino,
                          st_dev=stat_entry.st_dev,
                          st_nlink=stat_entry.st_nlink,
                          st_uid=stat_entry.st_uid,
                          st_gid=stat_entry.st_gid,
                          st_size=stat_entry.st_size,
                          st_atime=stat_entry.st_atime,
                          st_mtime=stat_entry.st_mtime,
                          st_ctime=stat_entry.st_ctime,
                          st_blocks=stat_entry.st_blocks,
                          st_blksize=stat_entry.st_blksize,
                          st_rdev=stat_entry.st_rdev,
                          symlink=stat_entry.symlink)

    hash_obj = vfs_file.Get(vfs_file.Schema.HASH)
    if hash_obj:
      StatEntryToExportedFileConverter.ParseFileHash(hash_obj, result)

    return [result]


class GrrMessageConverter(ExportConverter):
  """Converts GrrMessage's payload into a set of RDFValues.

  GrrMessageConverter converts given GrrMessages to a set of exportable
  RDFValues. It looks at the payload of every message and applies necessary
  converters to produce the resulting RDFValues.

  Usually, when a value is converted via one of the ExportConverter classes,
  metadata (ExportedMetadata object describing the client, session id, etc)
  are provided by the caller. But when converting GrrMessages, the caller can't
  provide any reasonable metadata. In order to understand where the messages
  are coming from, one actually has to inspect the messages source and this
  is done by GrrMessageConverter and not by the caller.

  Although ExportedMetadata should still be provided for the conversion to
  happen, only "source_urn" and value will be used. All other metadata will be
  fetched from the client object pointed to by GrrMessage.source.
  """

  input_rdf_type = "GrrMessage"

  def __init__(self, *args, **kw):
    super(GrrMessageConverter, self).__init__(*args, **kw)
    self.cached_metadata = {}

  def Convert(self, metadata, grr_message, token=None):
    """Converts GrrMessage into a set of RDFValues.

    Args:
      metadata: ExportedMetadata to be used for conversion.
      grr_message: GrrMessage to be converted.
      token: Security token.

    Returns:
      List or generator with resulting RDFValues.
    """
    return self.BatchConvert([(metadata, grr_message)], token=token)

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Converts a batch of GrrMessages into a set of RDFValues at once.

    Args:
      metadata_value_pairs: a list or a generator of tuples (metadata, value),
                            where metadata is ExportedMetadata to be used for
                            conversion and value is a GrrMessage to be
                            converted.
      token: Security token.

    Returns:
      Resulting RDFValues. Empty list is a valid result and means that
      conversion wasn't possible.
    """

    # Group messages by source (i.e. by client urn).
    msg_dict = {}
    for metadata, msg in metadata_value_pairs:
      msg_dict.setdefault(msg.source, []).append((metadata, msg))

    metadata_objects = []
    metadata_to_fetch = []

    # Open the clients we don't have metadata for and fetch metadata.
    for client_urn in msg_dict.iterkeys():
      try:
        metadata_objects.append(self.cached_metadata[client_urn])
      except KeyError:
        metadata_to_fetch.append(client_urn)

    if metadata_to_fetch:
      client_fds = aff4.FACTORY.MultiOpen(metadata_to_fetch, mode="r",
                                          token=token)
      fetched_metadata = [GetMetadata(client_fd, token=token)
                          for client_fd in client_fds]
      for metadata in fetched_metadata:
        self.cached_metadata[metadata.client_urn] = metadata
      metadata_objects.extend(fetched_metadata)

    data_by_type = {}
    for metadata in metadata_objects:
      try:
        for original_metadata, message in msg_dict[metadata.client_urn]:
          # Get source_urn from the original metadata provided and
          # original_timestamp from the payload age.
          new_metadata = rdfvalue.ExportedMetadata(metadata)
          new_metadata.source_urn = original_metadata.source_urn
          new_metadata.original_timestamp = message.payload.age
          cls_name = message.payload.__class__.__name__

          # Create a dict of values for conversion keyed by type, so we can
          # apply the right converters to the right object types
          if cls_name not in data_by_type:
            converters_classes = ExportConverter.GetConvertersByValue(
                message.payload)
            data_by_type[cls_name] = {
                "converters": [cls(self.options) for cls in converters_classes],
                "batch_data": [(new_metadata, message.payload)]}
          else:
            data_by_type[cls_name]["batch_data"].append(
                (new_metadata, message.payload))

      except KeyError:
        pass

    # Run all converters against all objects of the relevant type
    converted_batch = []
    for dataset in data_by_type.values():
      for converter in dataset["converters"]:
        converted_batch.extend(converter.BatchConvert(dataset["batch_data"],
                                                      token=token))

    return converted_batch


class FileStoreImageToExportedFileStoreHashConverter(ExportConverter):
  """Converts FileStoreImage to ExportedFileStoreHash."""

  input_rdf_type = "ExportedFileStoreImage"

  def Convert(self, metadata, stat_entry, token=None):
    """Converts StatEntry to ExportedFile.

    Does nothing if StatEntry corresponds to a registry entry and not to a file.

    Args:
      metadata: ExportedMetadata to be used for conversion.
      stat_entry: StatEntry to be converted.
      token: Security token.

    Returns:
      List or generator with resulting RDFValues. Empty list if StatEntry
      corresponds to a registry entry and not to a file.
    """
    return self.BatchConvert([(metadata, stat_entry)], token=token)

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Converts a batch of StatEntry value to ExportedFile values at once.

    Args:
      metadata_value_pairs: a list or a generator of tuples (metadata, value),
                            where metadata is ExportedMetadata to be used for
                            conversion and value is a StatEntry to be converted.
      token: Security token:

    Yields:
      Resulting ExportedFile values. Empty list is a valid result and means that
      conversion wasn't possible.
    """
    raise NotImplementedError()


class FileStoreHashConverter(ExportConverter):
  input_rdf_type = "FileStoreHash"

  def Convert(self, metadata, stat_entry, token=None):
    """Convert a single FileStoreHash."""

    return self.BatchConvert([(metadata, stat_entry)], token=token)

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Convert batch of FileStoreHashs."""

    urns = [urn for metadata, urn in metadata_value_pairs]
    urns_dict = dict([(urn, metadata)
                      for metadata, urn in metadata_value_pairs])

    results = []
    for hash_urn, client_files in filestore.HashFileStore.GetClientsForHashes(
        urns, token=token):
      for hit in client_files:
        metadata = rdfvalue.ExportedMetadata(urns_dict[hash_urn])
        metadata.client_urn = rdfvalue.RDFURN(hit).Split(2)[0]

        result = rdfvalue.ExportedFileStoreHash(
            metadata=metadata,
            hash=hash_urn.hash_value,
            fingerprint_type=hash_urn.fingerprint_type,
            hash_type=hash_urn.hash_type,
            target_urn=hit)
        results.append(result)

    return results


def GetMetadata(client, token=None):
  """Builds ExportedMetadata object for a given client id.

  Args:
    client: RDFURN of a client or VFSGRRClient object itself.
    token: Security token.

  Returns:
    ExportedMetadata object with metadata of the client.
  """

  if isinstance(client, rdfvalue.RDFURN):
    client_fd = aff4.FACTORY.Open(client, mode="r", token=token)
  else:
    client_fd = client

  metadata = ExportedMetadata()

  metadata.client_urn = client_fd.urn
  metadata.client_age = client_fd.urn.age

  metadata.hostname = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.HOSTNAME, u""))

  metadata.os = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.SYSTEM, u""))

  metadata.uname = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.UNAME, u""))

  metadata.os_release = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.OS_RELEASE, u""))

  metadata.os_version = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.OS_VERSION, u""))

  metadata.usernames = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.USERNAMES, u""))

  metadata.mac_address = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.MAC_ADDRESS, u""))

  client_info = client_fd.Get(client_fd.Schema.CLIENT_INFO)
  if client_info is not None:
    metadata.labels = u",".join(client_info.labels)

  return metadata


def ConvertValuesWithMetadata(metadata_value_pairs, token=None, options=None):
  """Converts a set of RDFValues into a set of export-friendly RDFValues.

  Args:
    metadata_value_pairs: Tuples of (metadata, rdf_value), where metadata is
                          an instance of ExportedMetadata and rdf_value is
                          an RDFValue subclass instance to be exported.
    token: Security token.
    options: rdfvalue.ExportOptions instance that will be passed to
             ExportConverters.
  Yields:
    Converted values. Converted values may be of different types.

  Raises:
    NoConverterFound: in case no suitable converters were found for a value in
                      metadata_value_pairs. This error is only raised after
                      all values in metadata_value_pairs are attempted to be
                      converted. If there are multiple value types that could
                      not be converted because of the lack of corresponding
                      converters, only the last one will be specified in the
                      exception message.
  """

  no_converter_found_error = None
  for rdf_type, metadata_values_group in utils.GroupBy(
      metadata_value_pairs,
      lambda pair: pair[1].__class__.__name__).iteritems():

    _ = rdf_type
    _, first_value = metadata_values_group[0]
    converters_classes = ExportConverter.GetConvertersByValue(first_value)
    if not converters_classes:
      no_converter_found_error = "No converters found for value: %s" % str(
          first_value)
      continue

    converters = [cls(options) for cls in converters_classes]
    for converter in converters:
      for result in converter.BatchConvert(metadata_values_group, token=token):
        yield result

  if no_converter_found_error is not None:
    raise NoConverterFound(no_converter_found_error)


def ConvertValues(default_metadata, values, token=None, options=None):
  """Converts a set of RDFValues into a set of export-friendly RDFValues.

  Args:
    default_metadata: rdfvalue.ExportedMetadata instance with basic
                      information about where the values come from.
                      This metadata will be passed to exporters.
    values: Values to convert. They should be of the same type.
    token: Security token.
    options: rdfvalue.ExportOptions instance that will be passed to
             ExportConverters.
  Returns:
    Converted values. Converted values may be of different types
    (unlike the source values which are all of the same type). This is due to
    the fact that multiple ExportConverters may be applied to the same value
    thus generating multiple converted values of different types.

  Raises:
    NoConverterFound: in case no suitable converters were found for the values.
  """

  batch_data = [(default_metadata, obj) for obj in values]
  return ConvertValuesWithMetadata(batch_data, token=token, options=options)
